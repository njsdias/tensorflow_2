{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Technique\n",
    "\n",
    "Ensemble Technique uses multiple models (multiple predictors) to construct a most strong model.\n",
    "\n",
    "Multiple models is a combination of models, for instance :\n",
    "- logistic regression + decision trees\n",
    "\n",
    "The ouputs of each predictor are combined by different averaging methods, such as:\n",
    "- weighted averages,\n",
    "- normal averages,\n",
    "- or votes, \n",
    "and a final prediction value is derived. \n",
    "\n",
    "The use of Ensemble methods is because the combination between models are more effective than individual methods and, therefore, are heavily used to build machine learning models. \n",
    "\n",
    "Ensemble methods can be implemented by either:\n",
    "- bagging\n",
    "- or boosting.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "The independent models/predictors are build using a random subsample/bootstrap of data for each of the models/\n",
    "predictors. \n",
    "\n",
    "Then an average (weighted, normal, or by voting) of the scores\n",
    "from the different predictors is taken to get the final score/prediction.\n",
    "\n",
    "The most famous bagging method is **Random Forest**.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "In boosting the predictors are not independently trained but done so in a sequential manner. \n",
    "\n",
    "For example, we\n",
    "build a logistic regression model on a subsample/bootstrap of the original\n",
    "training data set. Then we take the output of this model and feed it to a\n",
    "decision tree, to get the prediction, and so on. \n",
    "\n",
    "- input -> logistic regression -> output -> precious ouput is the input for -> decision tree -> output -> continues\n",
    "\n",
    "The aim of this sequential training is for the subsequent models to learn from the mistakes of the previous model. \n",
    "\n",
    "**Gradient Boosting** is an example of a boosting method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "The Gradient Boosting doesn't increment the weights of\n",
    "misclassified outcomes from one previous learner to the next. It optimize\n",
    "the loss function of the previous learner.\n",
    "\n",
    "Here demonstrate building a boosted trees classifier, using the gradient\n",
    "boosting method under the hood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset chosen was Iris dataset that is compounded by \n",
    "- features: \tSepalLength\tSepalWidth\tPentalLength\tPentalWidth\t\n",
    "- targets: Species\n",
    "\n",
    "The dataset has three types fo species. But only for demonstration (don't do multiclass classification) we will work only with two spcecies classes.\n",
    "\n",
    "### Data Transformation\n",
    "\n",
    "Through Exploratory Datat Analysis we saw the bimodal distribution in target due to we selected only two species. We can see the correlation between each feature with target.\n",
    "\n",
    "Data normalization was implemented, because it is a very recommend procedure when we have features with different value range.\n",
    "\n",
    "### TensorFlow Pipeline\n",
    "A TensorFlow pipeline was built to slice, shuffle and create batch of data for generate train and test dataset for Logistic model.\n",
    "\n",
    "### Model Verification\n",
    "To verify the model performance to make predictions of two different species was implemented: accuracy, precision, recall. The results were:\n",
    "- Training\n",
    "    - Training Data Accuracy (%) =  98.75\n",
    "    - Training Data Precision (%) =  97.73\n",
    "    - Training Data Recall (%) =  100.0\n",
    "- Test\n",
    "    - Test Data Accuracy (%) =  80.0\n",
    "    - Test Data Precision (%) =  63.64\n",
    "\n",
    "For this case we can evaluate the same model using F1 Score metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "\n",
    "from tensorflow.estimator import BoostedTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "The dataset is the well known Iris, that cathegorize four species of plants. The dataset is compound by features:\n",
    "- Sepal Length\n",
    "- Sepal Width\n",
    "- Petal Length\n",
    "- Petal Witdth\n",
    "\n",
    "and the categorical targets (species) are:\n",
    "- Setosa\n",
    "- Versicolor\n",
    "- Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns names\n",
    "col_names = ['SepalLength', 'SepalWidth', 'PentalLength', 'PentalWidth', 'Species']\n",
    "target_dimensions = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "# Load dataset\n",
    "# Define the path to read the dataset\n",
    "training_data_path = tf.keras.utils.get_file(\"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
    "test_data_path = tf.keras.utils.get_file(\"iris_test.csv\",\"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "\n",
    "# Train dataset\n",
    "# We are not interest in the original dataset header (header = 0)\n",
    "training = pd.read_csv(training_data_path, names=col_names, header=0)\n",
    "\n",
    "# Select the Species that not contains zero\n",
    "training = training[training['Species'] >= 1]\n",
    "\n",
    "# replace the Species values from 1,2 to 0,1\n",
    "training['Species'] = training['Species'].replace([1,2], [0,1])\n",
    "\n",
    "# Test dataset\n",
    "# We are not interest in the original dataset header (header = 0)\n",
    "test = pd.read_csv(test_data_path, names=col_names,header=0)\n",
    "\n",
    "# Select the Species that not contains zero\n",
    "test = test[test['Species'] >= 1]\n",
    "\n",
    "# Replace the Species values from 1,2 to 0,1\n",
    "test['Species'] = test['Species'].replace([1,2], [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SepalLength</th>\n",
       "      <td>100.0</td>\n",
       "      <td>6.262</td>\n",
       "      <td>0.662834</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.800</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.700</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SepalWidth</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.872</td>\n",
       "      <td>0.332751</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.025</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PentalLength</th>\n",
       "      <td>100.0</td>\n",
       "      <td>4.906</td>\n",
       "      <td>0.825578</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.375</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.525</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PentalWidth</th>\n",
       "      <td>100.0</td>\n",
       "      <td>1.676</td>\n",
       "      <td>0.424769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.300</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Species</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.502519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   mean       std  min    25%  50%    75%  max\n",
       "SepalLength   100.0  6.262  0.662834  4.9  5.800  6.3  6.700  7.9\n",
       "SepalWidth    100.0  2.872  0.332751  2.0  2.700  2.9  3.025  3.8\n",
       "PentalLength  100.0  4.906  0.825578  3.0  4.375  4.9  5.525  6.9\n",
       "PentalWidth   100.0  1.676  0.424769  1.0  1.300  1.6  2.000  2.5\n",
       "Species       100.0  0.500  0.502519  0.0  0.000  0.5  1.000  1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index of dataframes\n",
    "training.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate two dataframes through rows\n",
    "iris_dataset = pd.concat([training, test], axis=0)\n",
    "\n",
    "# Output stats \n",
    "iris_stats = iris_dataset.describe()\n",
    "\n",
    "# Put the dataframe in a better configuration to read\n",
    "iris_stats = iris_stats.transpose()\n",
    "\n",
    "# Show the statics results\n",
    "iris_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in Training Features:  80\n",
      "No. of rows in Test Features:  20\n",
      "No. of columns in Training Features:  4\n",
      "No. of columns in Test Features:  4\n",
      "No. of rows in Training Label:  80\n",
      "No. of rows in Test Label:  20\n",
      "No. of columns in Training Label:  1\n",
      "No. of columns in Test Label:  1\n"
     ]
    }
   ],
   "source": [
    "## Select the require dataset\n",
    "X_data = iris_dataset[[i for i in iris_dataset.columns if i not in ['Species']]]\n",
    "Y_data = iris_dataset[['Species']]\n",
    "\n",
    "# Split dataset in training and test\n",
    "training_features, test_features, training_labels, test_labels = train_test_split(X_data , Y_data , test_size=0.2)\n",
    "\n",
    "print('No. of rows in Training Features: ', training_features.shape[0])\n",
    "print('No. of rows in Test Features: ', test_features.shape[0])\n",
    "print('No. of columns in Training Features: ', training_features.shape[1])\n",
    "print('No. of columns in Test Features: ', test_features.shape[1])\n",
    "print('No. of rows in Training Label: ', training_labels.shape[0])\n",
    "print('No. of rows in Test Label: ', test_labels.shape[0])\n",
    "print('No. of columns in Training Label: ', training_labels.shape[1])\n",
    "print('No. of columns in Test Label: ', test_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize dataset\n",
    "def norm(x):\n",
    "    stats = x.describe()\n",
    "    stats = stats.transpose()\n",
    "    \n",
    "    return (x - stats['mean']) / stats['std']\n",
    "\n",
    "normed_train_features = norm(training_features)\n",
    "normed_test_features = norm(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the input pipeline for the TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_input(features_dataframe, target_dataframe, shuffle=True, num_of_epochs=10, batch_size=32):\n",
    "    # This function allows to shuffle dataset and create batchs of dataset for each epoch\n",
    "    def input_feed_function():\n",
    "        #  get the slices of an array in the form of objects\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(features_dataframe), target_dataframe))\n",
    "        \n",
    "        # Shuffle dataset in which X is the number of samples randomized \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(2000)\n",
    "            \n",
    "        dataset = dataset.batch(batch_size).repeat(num_of_epochs)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    return input_feed_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the train dataset to feed into Linear Model\n",
    "train_feed_input = feed_input(normed_train_features, training_labels)\n",
    "\n",
    "# In test We don't need shuffle dataset and the epoch is only one \n",
    "train_feed_input_testing = feed_input(normed_train_features, training_labels, num_of_epochs=1, shuffle=False)\n",
    "\n",
    "# Obtain the test dataset to feed into Linear Model\n",
    "test_feed_input = feed_input(normed_test_features, test_labels, num_of_epochs=1, shuffle=False)\n",
    "\n",
    "# Select numerical column for model\n",
    "feature_columns_numeric = [tf.feature_column.numeric_column(m) for m in training_features.columns]\n",
    "\n",
    "# Build Boost Decision Trees with categorical columns\n",
    "btree_model = BoostedTreesClassifier(feature_columns=feature_columns_numeric, n_batches_per_layer=1)\n",
    "\n",
    "# Train the model with dataset\n",
    "btree_model.train(train_feed_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpybj1_ru1/model.ckpt-29\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpybj1_ru1/model.ckpt-29\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with train dataset test\n",
    "train_predictions = btree_model.predict(train_feed_input_testing)\n",
    "\n",
    "# Make preditions with test dataset\n",
    "test_predictions = btree_model.predict(test_feed_input)\n",
    "\n",
    "# Transform train predictions into Series\n",
    "train_predictions_series = pd.Series([p['classes'][0].decode(\"utf-8\") for p in train_predictions])\n",
    "\n",
    "# Transform test Predictions in to series\n",
    "test_predictions_series = pd.Series([p['classes'][0].decode(\"utf-8\") for p in test_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Series into Dataframes\n",
    "train_predictions_df = pd.DataFrame(train_predictions_series, columns=['predictions'])\n",
    "test_predictions_df = pd.DataFrame(test_predictions_series, columns=['predictions'])\n",
    "\n",
    "# Reset indices before to join by row the two Dataframes\n",
    "training_labels.reset_index(drop=True, inplace=True)\n",
    "train_predictions_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_labels.reset_index(drop=True, inplace=True)\n",
    "test_predictions_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Join Dataframes\n",
    "train_labels_with_predictions_df = pd.concat([training_labels, train_predictions_df], axis=1)\n",
    "test_labels_with_predictions_df = pd.concat([test_labels,test_predictions_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics\n",
    "\n",
    "This part allow us to check the quality of our model. In the classification problems the metric that is usually used is: \n",
    "- accuracy\n",
    "- precision\n",
    "- recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy (%) =  98.75\n",
      "Training Data Precision (%) =  97.73\n",
      "Training Data Recall (%) =  100.0\n",
      "--------------------------------------------------\n",
      "Test Data Accuracy (%) =  80.0\n",
      "Test Data Precision (%) =  63.64\n"
     ]
    }
   ],
   "source": [
    "def calculate_binary_class_scores(y_true, y_pred):\n",
    "    # y_true : target original values\n",
    "    # y_pred : predict values\n",
    "    accuracy = accuracy_score(y_true,y_pred.astype('int64'))\n",
    "    precision = precision_score(y_true,y_pred.astype('int64'))\n",
    "    recall = recall_score(y_true, y_pred.astype('int64'))\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "\n",
    "train_accuracy_score, train_precision_score, train_recall_score = calculate_binary_class_scores(training_labels, train_predictions_series)\n",
    "test_accuracy_score, test_precision_score, test_recall_score = calculate_binary_class_scores(test_labels, test_predictions_series)\n",
    "print('Training Data Accuracy (%) = ', round(train_accuracy_score*100,2))\n",
    "print('Training Data Precision (%) = ', round(train_precision_score*100,2))\n",
    "print('Training Data Recall (%) = ', round(train_recall_score*100,2))\n",
    "print('-'*50)\n",
    "print('Test Data Accuracy (%) = ', round(test_accuracy_score*100,2))\n",
    "print('Test Data Precision (%) = ', round(test_precision_score*100,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
