{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks with TensorFlow 2.0\n",
    "\n",
    "Using the Keras API with TensorFlow, we will be building a simple neural\n",
    "network with only one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Here we will use fashion-MNIST data set by Zalando that contains\n",
    "- 70,000 images = 60,000 for training + 10,000 for testing\n",
    "- 10 categories\n",
    "    - t-shirt / top\n",
    "    - trouser\n",
    "    - pullover\n",
    "    - dress\n",
    "    - coat\n",
    "    - sandal\n",
    "    - sneaker\n",
    "    - bag\n",
    "    - ankle boot\n",
    "\n",
    "The images are 28 Ã— 28 pixels of individual articles of clothing,\n",
    "with values ranging from 0 to 255.\n",
    "\n",
    "Dataset: https://bit.ly/2xqIwCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fashion-MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, test_labels)=ks.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images Dataset Shape: (60000, 28, 28)\n",
      "No. of Training Images Dataset Labels: 60000\n",
      "Test Images Dataset Shape: (10000, 28, 28)\n",
      "No. of Test Images Dataset Labels: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Training Images Dataset Shape: {}'.format(training_images.shape))\n",
    "print('No. of Training Images Dataset Labels: {}'.format(len(training_labels)))\n",
    "print('Test Images Dataset Shape: {}'.format(test_images.shape))\n",
    "print('No. of Test Images Dataset Labels: {}'.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize dataset\n",
    "\n",
    "As the pixel values range from 0 to 255, we have to rescale these values\n",
    "in the range 0 to 1 before pushing them to the model. We can scale these\n",
    "values (both for training and test data sets) by dividing the values by 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build layers of Neural Network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Flatten)        (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_layer (Dense)         (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data_shape = (28,28)\n",
    "hidden_activation_function = 'relu'\n",
    "output_activation_function = 'softmax'\n",
    "\n",
    "# Instantiate Model\n",
    "nn_model = ks.models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model.add(ks.layers.Flatten(input_shape=input_data_shape, name='Input_Layer'))\n",
    "\n",
    "# Hidden Layer\n",
    "nn_model.add(ks.layers.Dense(32, activation=hidden_activation_function, name='Hidden_layer'))\n",
    "\n",
    "# Output Layer\n",
    "nn_model.add(ks.layers.Dense(10, activation=output_activation_function, name='Output_Layer'))\n",
    "\n",
    "# Build Model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model\n",
    "\n",
    "To optimze the loss/objective function (cross entropy) we chose Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5535 - accuracy: 0.8098\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4190 - accuracy: 0.8529\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3834 - accuracy: 0.8639\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3618 - accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3462 - accuracy: 0.8751\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3305 - accuracy: 0.8821\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3210 - accuracy: 0.8845\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3122 - accuracy: 0.8870\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3042 - accuracy: 0.8893\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2970 - accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbbc35cd278>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = 'Adam'\n",
    "loss_function = 'sparse_categorical_crossentropy' \n",
    "metric = ['accuracy']\n",
    "\n",
    "# Model Compile\n",
    "nn_model.compile(optimizer=optimizer, loss= loss_function, metrics=metric)\n",
    "\n",
    "# Model fit. Number of total fit cicles is set up with epochs parameter\n",
    "nn_model.fit(training_images, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3027 - accuracy: 0.8909\n",
      "Training Data Accuracy 0.89\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3875 - accuracy: 0.8646\n",
      "Test Data Accuracy 0.86\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with training dataset\n",
    "# it is expected have similary accuracy of last epoch\n",
    "training_loss, training_accuracy =  nn_model.evaluate(training_images,training_labels)\n",
    "print('Training Data Accuracy {}'.format(round(float(training_accuracy),2)))\n",
    "\n",
    "# Evaluation with test dataset: \n",
    "# It is expected have a litle bit low accuracy \n",
    "# than the evaluation with training dataset\n",
    "test_loss, test_accuracy = nn_model.evaluate(test_images, test_labels)\n",
    "print('Test Data Accuracy {}'.format(round(float(test_accuracy),2)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
