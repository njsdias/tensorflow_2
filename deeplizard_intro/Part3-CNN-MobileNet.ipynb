{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Nets\n",
    "\n",
    "Mobile NEts are a class of small low-power low latency models thar can be used for things like:\n",
    "- classification\n",
    "- detection\n",
    "\n",
    "Because of their small size thse models are considered great for mobile devices.\n",
    "\n",
    "Model | Size MB | Parameters (Million) | Accuracy\n",
    "----| ----|----| -----\n",
    "VGG16| 533 | 138 | High\n",
    "Mobile Net |17 | 4.2| Almost High\n",
    "    \n",
    "Paper that compares the MobileNet accuracy, [here](https://arxiv.org/pdf/1704.04861.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "mobile = tf.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the images on the path specified insie of this function\n",
    "def prepare_image(file):\n",
    "    \n",
    "    # path of where you saved your images\n",
    "    img_path = 'data/MobileNet-samples/'\n",
    "    # pich the image and resize it to (224,224)\n",
    "    img = image.load_img(img_path + file, target_size=(224, 224))\n",
    "    # tranform the image into an array format\n",
    "    img_array = image.img_to_array(img)\n",
    "    # expand the dimensions required by MobileNet miodel\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # return the image preprocessed by the MobileNet model\n",
    "    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Images and analyse model Prediction Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image from our image folder\n",
    "from IPython.display import Image\n",
    "Image(filename='data/MobileNet-samples/1.PNG', width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass that image to the MobileNet preprocess\n",
    "preprocessed_image = prepare_image'(1.PNG')\n",
    "\n",
    "# Make a prediction using the model\n",
    "predictions = mobile.predict(preprocessed_image)\n",
    "\n",
    "# save the results\n",
    "results= imagenet_utils.decode_predictions(predictions)\n",
    "\n",
    "# display results\n",
    "results\n",
    "\n",
    "# check the prediction is corrected\n",
    "assert results[0][0][1] == 'American_chameleon'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/MobileNet-samples/2.PNG', width=300)\n",
    "\n",
    "# Pass that image to the MobileNet preprocess\n",
    "preprocessed_image = prepare_image'(2.PNG')\n",
    "\n",
    "# Make a prediction using the model\n",
    "predictions = mobile.predict(preprocessed_image)\n",
    "\n",
    "# save the results\n",
    "results= imagenet_utils.decode_predictions(predictions)\n",
    "\n",
    "# display results\n",
    "results\n",
    "\n",
    "# check the prediction is corrected\n",
    "assert results[0][0][1] == 'espresso'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='data/MobileNet-samples/3.PNG', width=300)\n",
    "\n",
    "# Pass that image to the MobileNet preprocess\n",
    "preprocessed_image = prepare_image'(3.PNG')\n",
    "\n",
    "# Make a prediction using the model\n",
    "predictions = mobile.predict(preprocessed_image)\n",
    "\n",
    "# save the results\n",
    "results= imagenet_utils.decode_predictions(predictions)\n",
    "\n",
    "# display results\n",
    "results\n",
    "\n",
    "# check the prediction is corrected\n",
    "assert results[0][0][1] == 'strawberry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet Fine-Tune\n",
    "\n",
    "The dataset that we are working here is very different from the dataset the MobileNet was trained which used the ImageNet library. \n",
    "\n",
    "We are working with a dataset of images of sign language digits. here we have ten classes from 0 to 9. Each classes containes images of the aprticular sign for that digit. The images are grayscale\n",
    "\n",
    "Download dataset here()\n",
    "- [kaggle](https://www.kaggle.com/ardamavi/sign-language-digits-dataset): grayscale images\n",
    "- [github](https://github.com/ardamavi/Sign-Language-Digits-Dataset): RGB images (we'll use this)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preparation\n",
    "\n",
    "- 10 classes(digits 0-9)\n",
    "- Class data:\n",
    "    - class 0: 205 images\n",
    "    - class 1: 206 images\n",
    "    - class 2: 206 images\n",
    "    - class 3: 206 images\n",
    "    - class 4: 207 images\n",
    "    - class 5: 207 images\n",
    "    - class 6: 207 images\n",
    "    - class 7: 206 images\n",
    "    - class 8: 208 images\n",
    "    - class 9: 204 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## organzie data intro train, valid, test dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the folder path into memory\n",
    "os.chdir('data/Sign-Language-Digits-Dataset')\n",
    "\n",
    "# If not find the folder create folders in disk\n",
    "if os.path.isdir('train/0') is False:\n",
    "    os.mkdir('train')\n",
    "    os.mkdir('valid')\n",
    "    os.mkdir('test')\n",
    "    \n",
    "    # loop for each class\n",
    "    for i in range(0,10):\n",
    "        # move each class data to train\n",
    "        shutil.move(f'{i}', 'train')\n",
    "        # create a folder for each class\n",
    "        os.mkdir(f'valid/{i}')\n",
    "        os.mkdir(f'test/{i}')\n",
    "        \n",
    "        # take randomly 30 records from train and move it on validation folder\n",
    "        valid_samples = random.sample(os.listdir(f'train/{i}'), 30)\n",
    "        for j in valid_samples: \n",
    "            shutil.move(f'train/{i}/{j}', f'valid/{i}')\n",
    "        \n",
    "        # take randomly 5 records from train and move it on validationtest folder\n",
    "        test_samples = random.sample(os.listdir(f'train/{i}'), 5)\n",
    "        for j in test_samples: \n",
    "            shutil.move(f'train/{i}/{j}', f'valid/{i}')\n",
    "            \n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of files in each folder\n",
    "for i in range(0,10):\n",
    "    assert len(os.listdir(f'data/Sign-Language-Digits-Dataset/valid/{i}')) == 30\n",
    "    assert len(os.listdir(f'data/Sign-Language-Digits-Dataset/test/{i}')) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/Sign-Language-Digits-Dataset/train'\n",
    "valid_path = 'data/Sign-Language-Digits-Dataset/valid'\n",
    "test_path = 'data/Sign-Language-Digits-Dataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=train_path, target_size=(224,224), batch_size=10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=valid_path, target_size=(224,224), batch_size=10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if we have the right amount of images\n",
    "assert train_batches-n == 1712\n",
    "assert train_batches-n == 300\n",
    "assert train_batches-n == 50\n",
    "assert train_batches.num_classes = valid_batches.num_classes = test_batches.num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download moel to your disk : requires internt connection\n",
    "mobile = tf.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model architecture \n",
    "mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model is corrected downloaded evaluating some model parameters\n",
    "params = count_params(mobile)\n",
    "assert params['non_trainable_params'] == 21888\n",
    "assert params['trainable_params'] == 4231976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fune-Tuning process\n",
    "\n",
    "This process start out with us getting all of the layers up to the 6th to last layer. So the last five layers are not included\n",
    "\n",
    "So all these layers are what we are going to keep and transfer into a new model (new fine-tuned model) and we are not going to include the last FIVE layers.\n",
    "\n",
    "This is a choice after a tille experimentating and testing the number of layers that you choose to include versus not include whenever you're fine tunning a model is going to come through experimentation and personal choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the layers of interest\n",
    "x = mobile.layers[-6].output\n",
    "# add thoses layers to the output layer. \n",
    "# This is a functional model and this is why it can appear a litle strange\n",
    "output = Dense(units=10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the original input of the model and the output is the output that we already specified \n",
    "model = Model(inputs=model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the weights and the bias of the model because we don't want to train again the model\n",
    "# the choice to train the last 23 layers was achieved by experimentation\n",
    "for layer in model.layers[:-23]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = count_params(model)\n",
    "assert params['non_trainable_params'] == 1365184\n",
    "assert params['trainable_params'] == 1873930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit model to our data\n",
    "# run for more epochs (~30) to see better results\n",
    "model.fit(x=train_batches, validation=valid_batches, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements that we can do:\n",
    "- change the number of freeze layers\n",
    "- change the epochs number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicts sign language digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tets_lables = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=test_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for more epochs to see better results\n",
    "cm_plot_results = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
