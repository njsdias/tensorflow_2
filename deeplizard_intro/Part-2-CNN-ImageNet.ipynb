{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification: dogs vs cats\n",
    "\n",
    "Download Dataset [here](https://www.kaggle.com/c/dogs-vs-cats/data)\n",
    "\n",
    "**Organize the dataset**\n",
    " \n",
    "Download datset and delete test.zip and csv files. We don't need it in this example.\n",
    "Make sure all images are inside of one fodler like:\n",
    " \n",
    "         /data/dogs-vs-cats/train/\n",
    "         \n",
    "The recommendation here is try to run the code using a GPU to make it fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The images included in the data/cats-and-dogs directory are a random subset of the full cat and dog data set from the Kaggle competion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data into train, valid, test dirs\n",
    "os.chdir('data/dogs-vs-cats')\n",
    "if os.path.isdir('train/dog') is False\n",
    "    os.makedirs('train/dog')\n",
    "    os.makedirs('train/cat')\n",
    "    os.makedirs('valid/dog')\n",
    "    os.makedirs('valid/cat')\n",
    "    os.makedirs('test/dog')\n",
    "    os.makedirs('test/cat')\n",
    "    \n",
    "    # Train dataset\n",
    "    for c in random.sample(glob.glob('cat*'),500):\n",
    "        shutil.move(c, 'train/cat')\n",
    "    for c in random.sample(glob.glob('dog*'),500):\n",
    "        shutil.move(c, 'train/dog')\n",
    "    \n",
    "    # Validation set\n",
    "    for c in random.sample(glob.glob('cat*'),100):\n",
    "        shutil.move(c, 'valid/cat')\n",
    "    for c in random.sample(glob.glob('dog*'),100):\n",
    "        shutil.move(c, 'valid/dog')\n",
    "    \n",
    "    # Test set\n",
    "    for c in random.sample(glob.glob('cat*'),50):\n",
    "        shutil.move(c, 'test/cat')\n",
    "    for c in random.sample(glob.glob('dog*'),50):\n",
    "        shutil.move(c, 'test/dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/dogs-vs-cats/train'\n",
    "valid_path = 'data/dogs-vs-cats/valid'\n",
    "test_path = 'data/dogs-vs-cats/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the data format suitable to Keras\n",
    "\n",
    "Here we are processing the images in the same format the VGG16 model process. \n",
    "\n",
    "**Suffle is False in Test Batches**\n",
    "\n",
    "It is because whenever we use our test batches later for inference to get our model to predict on images of cats and dogs after training and validation has been completed we are going to want to look at out prediction results in a confusion matrix and in order to do that we need to be able to access the unsheffuled labels for our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=valid_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification that we have the correct amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_bataches.n == 1000\n",
    "assert valid_bataches.n == 200\n",
    "assert test_bataches.n == 100\n",
    "assert train_batches.num_classes = valid_batches.num_classes = test_batches.num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call next(train_batches) to generate a batch of images and labels from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get 10 images and 10 correspnd lables\n",
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImages(imgs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN using Tensorflow and Keras API\n",
    "\n",
    "**[Zero padding](https://deeplizard.com/learn/video/qSTv_m-KFk0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(224,224,3)),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(), # flatt alll of this into 1-D tensor \n",
    "    Dense(units=2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't specify the labels because when the data is stored as generator as a generator, the generator itself actually contains the corresponding lables, so we don't need to specify them separately whenever we call fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The training accuracy is much higher than validation accuracy thats means the model not generalized well and we have overfitting in training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, test_lables = next(test_batches)\n",
    "plotImages(test_imgs)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=test_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting predictions output**\n",
    "\n",
    "- [1., 0.] : it means the model predict the class in ZERO index that has the value 1. -> CAT\n",
    "- [0., 1.] : it means the model predict the class in ONE index that has the value 1. -> DOG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmx(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels to use in the right sequence for our confusion matrix\n",
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['cat', 'dog']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tunning to increase the performance of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we uses here is VGG16 that won's the ImageNet Compettion at 2014.\n",
    "\n",
    "The ImageNet library is made up of thousands of images that belong to one thousand of different classes.\n",
    "\n",
    "Using Keras we will import this VGG16 model and then fine tune it to not classify on one of the 1000 categories for which it was originally trained but instead only on two categories, cat and dog. \n",
    "\n",
    "Note, however that cats and dogs were included in the original image net library for wich VGG16 was trained on and becasue of this we sont have to do much tunning to change the model from classyfing from 1000 classes to  just the to cat and dog classes. So the overal gine-tunning the we'll do will be very minimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model - Internete conenction nedded\n",
    "vgg16_model = tf.keras.application.vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the mdeol architecture\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the alst layer (output layer) the model was built to rpedict 1000 classe.s Our work here is modify this output layer to model predict only two output classes correspondig cat and dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure we import the model in correct way\n",
    "def count_params(model):\n",
    "    non_trainable_params = np.sum([np.prod(v.get_shapr(.as_list()) for v in model.non_trainable_weights)])\n",
    "    trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_weights])\n",
    "    return {'non_trainable_params': non_trainable_params, 'trainable_params': trainable_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = count(vgg16_model)\n",
    "assert params['non_trainable_params'] == 0\n",
    "assert aprams['trainable_params'] == 138357544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the type of vgg16_model is Model type\n",
    "type(vgg16_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert Model type into Sequential type.\n",
    "\n",
    "Next we are loop all layer, except the last one (prediction layer), and add each layer in our Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why FREZZ Weights and Biases?**\n",
    "\n",
    "Now, we need to inform our model that the layers that are in the model are not trainable layers. In other words we are saying, don't touch on weights and biases values thar are in layers because they're not going to be retrained whenever we go through the training process for cats and dogs. \n",
    "\n",
    "Becasue VGG16 has already learned features of cats and dogs in its original training we don't wanted to have to go through more training again since it's already learned those features. This is the reason to freez the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add our output layer** to this model. Remember we've removed the previous output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to confirm the classes of the otput layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure that our model was well build\n",
    "params = count_params(model)\n",
    "assert params['non_trainable_params'] == 134260544\n",
    "assert params['trainable_params'] == 8194"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit model to our data\n",
    "model.fit(x=train_batches, validation=valid_batches, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duscussion Results**\n",
    "\n",
    "We get more accurated results with this new model. It is not totally surprising because the VGG16 had already been trained on images of cats and dogs from ImageNet library. So, the model had already those features. \n",
    "\n",
    "The improvement was gained when we told the VGG16 model to classify tje images only in two categories, cat or dog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.history.history.get('accuracy')[-1] > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=test_batchs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['cat', 'dog']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
